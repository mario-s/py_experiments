{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentimental Analysis\n",
    "This notebook shows how to create a model to analyze the mood of an user by his typed text. It is based on excelennt tutorial about classifying tweets: https://vgpena.github.io/classifying-tweets-with-keras-and-tensorflow/\n",
    "\n",
    "The dataset can be downloaded here: http://thinknook.com/wp-content/uploads/2012/09/Sentiment-Analysis-Dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import keras\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import Dense, Dropout, Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common constants for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'datasets/sentimental/'\n",
    "model_weights = 'model.h5'\n",
    "json_model_file = 'model.json'\n",
    "\n",
    "# only work with the 3000 most popular words found in the dataset\n",
    "max_words = 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The building of the model can be skipped if it was done before, and the model file and weights  have been saved. However retrain on a new dataset is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_array(dictionary, text):\n",
    "    \"\"\" \n",
    "    make all texts the same length -- in this case, the length\n",
    "    of the longest text in the set.\n",
    "    \"\"\"\n",
    "    return [dictionary[word] for word in kpt.text_to_word_sequence(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(max_words, train_x, train_y):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=(max_words,), activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(256, activation='sigmoid'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(train_x, train_y, batch_size=32, epochs=5, verbose=1,validation_split=0.1, shuffle=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_data():\n",
    "    # extract data from a csv\n",
    "    training = np.genfromtxt(path + 'sentiment-analysis.csv', delimiter=',', skip_header=1, \n",
    "                         usecols=(1, 3), dtype=None, encoding=None)\n",
    "    # create training data\n",
    "    train_x = [x[1] for x in training]\n",
    "    # index all the labels\n",
    "    train_y = np.asarray([x[0] for x in training])\n",
    "    return train_x, train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary(tokenizer):\n",
    "    # Tokenizers come with a convenient list of words and IDs\n",
    "    dictionary = tokenizer.word_index\n",
    "    # save this out so we can use it later\n",
    "    with open(path + 'dictionary.json', 'w') as dictionary_file: \n",
    "        json.dump(dictionary, dictionary_file)\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = training_data()\n",
    "# create a new Tokenizer\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "# feed the tweets to the Tokenizer\n",
    "tokenizer.fit_on_texts(train_x)\n",
    "dictionary = create_dictionary(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each tweet, change each token to its ID in the Tokenizer's word_index\n",
    "allWordIndices = []\n",
    "for text in train_x:\n",
    "    wordIndices = text_to_array(dictionary, text)\n",
    "    allWordIndices.append(wordIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create matrices out of the indexed tweets\n",
    "train_X = tokenizer.sequences_to_matrix(allWordIndices, mode='binary')\n",
    "# treat the labels as categories\n",
    "train_Y = keras.utils.to_categorical(train_y, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1420764 samples, validate on 157863 samples\n",
      "Epoch 1/5\n",
      "1420764/1420764 [==============================] - 2078s 1ms/step - loss: 0.4951 - acc: 0.7609 - val_loss: 0.4550 - val_acc: 0.7867\n",
      "Epoch 2/5\n",
      "1420764/1420764 [==============================] - 31361s 22ms/step - loss: 0.4742 - acc: 0.7759 - val_loss: 0.4469 - val_acc: 0.7910\n",
      "Epoch 3/5\n",
      "1420764/1420764 [==============================] - 51131s 36ms/step - loss: 0.4663 - acc: 0.7818 - val_loss: 0.4507 - val_acc: 0.7901\n",
      "Epoch 4/5\n",
      "1420764/1420764 [==============================] - 24281s 17ms/step - loss: 0.4613 - acc: 0.7855 - val_loss: 0.4456 - val_acc: 0.7921\n",
      "Epoch 5/5\n",
      "1420764/1420764 [==============================] - 14368s 10ms/step - loss: 0.4570 - acc: 0.7886 - val_loss: 0.4438 - val_acc: 0.7930\n"
     ]
    }
   ],
   "source": [
    "model = build_model(max_words, train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model and weights\n",
    "json_model = model.to_json()\n",
    "with open(path + json_model_file, 'w') as json_file:\n",
    "    json_file.write(json_model)\n",
    "\n",
    "model.save_weights(path + model_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the model to classify an input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to use the model to classify an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_index_array(text):\n",
    "    \"\"\" \n",
    "    make sure that all the words in your input\n",
    "    are registered in the dictionary\n",
    "    before trying to turn them into a matrix.\n",
    "    \"\"\"\n",
    "    words = kpt.text_to_word_sequence(text)\n",
    "    wordIndices = []\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            wordIndices.append(dictionary[word])\n",
    "        else:\n",
    "            print(\"'%s' not in training data; ignoring.\" %(word))\n",
    "    return wordIndices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_sentence(sentence, tokenizer, model):\n",
    "    if len(sentence) > 0:\n",
    "        # format input for the neural net\n",
    "        testArr = words_to_index_array(sentence)\n",
    "        input = tokenizer.sequences_to_matrix([testArr], mode='binary')\n",
    "        # predict which bucket input belongs in\n",
    "        pred = model.predict(input)\n",
    "        # for human-friendly printing\n",
    "        labels = ['negative', 'positive']\n",
    "        print(\"%s sentiment; %f%% confidence\" % (labels[np.argmax(pred)], pred[0][np.argmax(pred)] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following step can be skipped, if the model was just trained before and is still in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in saved dictionary\n",
    "with open(path + 'dictionary.json', 'r') as dictionary_file:\n",
    "    dictionary = json.load(dictionary_file)\n",
    "    \n",
    "# read saved model structure\n",
    "json_file = open(path + json_model_file, 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "\n",
    "# create a model from that\n",
    "model = model_from_json(loaded_model_json)\n",
    "# weight nodes with saved values\n",
    "model.load_weights(path + model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new tokenizer\n",
    "tokenizer = Tokenizer(num_words=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive sentiment; 79.433322% confidence\n"
     ]
    }
   ],
   "source": [
    "eval_sentence('What a nice day', tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive sentiment; 66.858613% confidence\n"
     ]
    }
   ],
   "source": [
    "eval_sentence('I like to kiss you', tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative sentiment; 90.897590% confidence\n"
     ]
    }
   ],
   "source": [
    "eval_sentence('This product is crap.', tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
