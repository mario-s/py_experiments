{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial neural networks \n",
    "In this section we will look at what is arguably the most common type of artificial neural network, a feed-forward network with backpropagation. Feed-forward means the signal is generally moving in one direction through the network. Backpropagation means we will determine errors at the end of each signal’s traversal through the network and try to distribute fixes for those errors back through the network, especially affecting the neurons that were most responsible for them.\n",
    "## Neurons\n",
    "The smallest unit in an artificial neural network is a neuron. It holds a vector of weights, which are just floating-point numbers. A vector of inputs (also just floating-point numbers) is passed to the neuron. It combines those inputs with its weights using a dot product. It then runs an activation function on that product and spits the result out as its output. This action can be thought of as analagous to a real neuron firing.\n",
    "An activation function is a transformer of the neuron’s output. The activation function is almost always nonlinear, which allows neural networks to represent solutions to nonlinear problems. \n",
    "## Layers\n",
    "In a typical feed-forward artificial neural network, neurons are organized in layers. Each layer consists of a certain number of neurons lined up in a row or column. In a feed-forward network, which is what we will be building, signals always pass in a single direction from one layer to the next. The neurons in each layer send their output signal to be used as input to the neurons in the next layer. Every neuron in each layer is connected to every neuron in the next layer. \n",
    "The first layer is known as the input layer, and it receives its signals from some external entity. The last layer is known as the output layer, and its output typically must be interpreted by an external actor to get an intelligent result. The layers between the input and output layers are known as hidden layers. \n",
    "Imagine that the network was designed to classify small black-and-white images of animals. Perhaps the input layer has 100 neurons representing the grayscale intensity of each pixel in a 10 x 10 pixel animal image, and the output layer has 5 neurons representing the likelihood that the image is of a mammal, reptile, amphibian, fish, or bird. The final classification could be determined by the output neuron with the highest floating-point output. If the output numbers were 0.24, 0.65, 0.70, 0.12, and 0.21, respectively, the image would be determined to be an amphibian.\n",
    "## Backpropagation\n",
    "Backpropagation finds the error in a neural network’s output and uses it to modify the weights of neurons. The neurons most responsible for the error are most heavily modified.\n",
    "Before they can be used, most neural networks must be trained. We must know the right outputs for some inputs so that we can use the difference between expected outputs and actual outputs to find errors and modify weights. In other words, neural networks know nothing until they are told the right answers for a certain set of inputs, so that they can prepare themselves for other inputs. Backpropagation only occurs during training.\n",
    "## Dot product\n",
    "As you will recall, dot products are required both for the feed-forward phase and for the backpropagation phase. Luckily, a dot product is simple to implement using the Python built-in functions zip() and sum()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from math import exp\n",
    "\n",
    "# dot product of two vectors\n",
    "def dot_product(xs: List[float], ys: List[float]) -> float:\n",
    "    return sum(x * y for x, y in zip(xs, ys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The activation function \n",
    "The activation function has two purposes: It allows the neural network to represent solutions that are not just linear transformations (as long as the activation function itself is not just a linear transformation), and it can keep the output of each neuron within a certain range. An activation function should have a computable derivative so that it can be used for backpropagation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the classic sigmoid activation function\n",
    "def sigmoid(x: float) -> float:\n",
    "    return 1.0 / (1.0 + exp(-x))\n",
    "\n",
    "def derivative_sigmoid(x: float) -> float:\n",
    "    sig: float = sigmoid(x)\n",
    "    return sig * (1 - sig) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing neurons \n",
    "An individual neuron will store many pieces of state, including its weights, its delta, its learning rate, a cache of its last output, and its activation function, along with the derivative of that activation function. Some of these elements could be more efficiently stored up a level (in the future Layer class), but they are included in the following Neuron class for illustrative purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Callable\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, weights: List[float], learning_rate: float,\n",
    "     activation_function: Callable[[float], float], derivative_activation_function: \n",
    "                 Callable[[float], float]) -> None:\n",
    "        self.weights: List[float] = weights\n",
    "        self.activation_function: Callable[[float], float] = activation_function\n",
    "        self.derivative_activation_function: Callable[[float], float] = derivative_activation_function\n",
    "        self.learning_rate: float = learning_rate\n",
    "        self.output_cache: float = 0.0\n",
    "        self.delta: float = 0.0\n",
    "\n",
    "    def output(self, inputs: List[float]) -> float:\n",
    "        self.output_cache = dot_product(inputs, self.weights)\n",
    "        return self.activation_function(self.output_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing layers \n",
    "A layer in our network will need to maintain three pieces of state: its neurons, the layer that preceded it, and an output cache. The output cache is similar to that of a neuron, but up one level. It caches the outputs (after activation functions are applied) of every neuron in the layer.\n",
    "\n",
    "At creation time, a layer’s main responsibility is to initialize its neurons. Our Layer class’s __init__() method therefore needs to know how many neurons it should be initializing, what their activation functions should be, and what their learning rates should be. In this simple network, every neuron in a layer has the same activation function and learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import List, Callable, Optional\n",
    "from random import random\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, previous_layer: Optional[Layer], num_neurons: int, learning_rate: float, \n",
    "                 activation_function: Callable[[float], float], \n",
    "                 derivative_activation_function: Callable[[float], float]) -> None:\n",
    "        \n",
    "        self.previous_layer: Optional[Layer] = previous_layer\n",
    "        self.neurons: List[Neuron] = []\n",
    "        \n",
    "            \n",
    "        # the following could all be one large list comprehension \n",
    "        for i in range(num_neurons):\n",
    "            if previous_layer is None:\n",
    "                random_weights: List[float] = []\n",
    "            else:\n",
    "                random_weights = [random() for _ in range(len(previous_layer.neurons))]\n",
    "            neuron: Neuron = Neuron(random_weights, learning_rate, activation_function, \n",
    "                                    derivative_activation_function)\n",
    "            self.neurons.append(neuron)\n",
    "            \n",
    "        self.output_cache: List[float] = [0.0 for _ in range(num_neurons)]\n",
    "            \n",
    "    def outputs(self, inputs: List[float]) -> List[float]:\n",
    "        if self.previous_layer is None:\n",
    "            self.output_cache = inputs\n",
    "        else:\n",
    "            self.output_cache = [n.output(inputs) for n in self.neurons]\n",
    "        return self.output_cache\n",
    "    \n",
    "    # should only be called on output layer\n",
    "    def calculate_deltas_for_output_layer(self, expected: List[float]) -> None:\n",
    "        for n in range(len(self.neurons)):\n",
    "            self.neurons[n].delta = self.neurons[n].derivative_activation_function(\n",
    "                    self.neurons[n].output_cache) * (expected[n] - self.output_cache[n])\n",
    "\n",
    "    # should not be called on output layer\n",
    "    def calculate_deltas_for_hidden_layer(self, next_layer: Layer) -> None:\n",
    "        for index, neuron in enumerate(self.neurons):\n",
    "            next_weights: List[float] = [n.weights[index] for n in next_layer.neurons]\n",
    "            next_deltas: List[float] = [n.delta for n in next_layer.neurons]\n",
    "            sum_weights_and_deltas: float = dot_product(next_weights, next_deltas)\n",
    "            neuron.delta = neuron.derivative_activation_function(neuron.output_cache) * sum_weights_and_deltas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the network \n",
    "The network itself has only one piece of state: the layers that it manages. The Network class is responsible for initializing its constituent layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import List, Callable, TypeVar, Tuple\n",
    "from functools import reduce\n",
    "\n",
    "T = TypeVar('T') # output type of interpretation of neural network\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, layer_structure: List[int], learning_rate: float,\n",
    "     activation_function: Callable[[float], float] = sigmoid, \n",
    "                 derivative_activation_function: Callable[[float], float] = derivative_sigmoid) -> None:\n",
    "        if len(layer_structure) < 3:\n",
    "            raise ValueError(\"Error: Should be at least 3 layers (1 input, 1 hidden, 1 output)\")\n",
    "        self.layers: List[Layer] = []\n",
    "        # input layer\n",
    "        input_layer: Layer = Layer(None, layer_structure[0], learning_rate, \n",
    "                                   activation_function, derivative_activation_function)\n",
    "        self.layers.append(input_layer)\n",
    "        # hidden layers and output layer\n",
    "        for previous, num_neurons in enumerate(layer_structure[1::]):\n",
    "            next_layer = Layer(self.layers[previous], num_neurons, learning_rate, \n",
    "                               activation_function, derivative_activation_function)\n",
    "            self.layers.append(next_layer)\n",
    "            \n",
    "    # Pushes input data to the first layer, then output from the first\n",
    "    # as input to the second, second to the third, etc.\n",
    "    def outputs(self, input: List[float]) -> List[float]:\n",
    "        return reduce(lambda inputs, layer: layer.outputs(inputs), self.layers, input)\n",
    "    \n",
    "    # Figure out each neuron's changes based on the errors of the output\n",
    "    # versus the expected outcome\n",
    "    def backpropagate(self, expected: List[float]) -> None:\n",
    "        # calculate delta for output layer neurons\n",
    "        last_layer: int = len(self.layers) - 1\n",
    "        self.layers[last_layer].calculate_deltas_for_output_layer(expected)\n",
    "        # calculate delta for hidden layers in reverse order\n",
    "        for l in range(last_layer - 1, 0, -1):\n",
    "            self.layers[l].calculate_deltas_for_hidden_layer(self.layers[l + 1])\n",
    "            \n",
    "    # backpropagate() doesn't actually change any weights\n",
    "    # this function uses the deltas calculated in backpropagate() to\n",
    "    # actually make changes to the weights\n",
    "    def update_weights(self) -> None:\n",
    "        for layer in self.layers[1:]: # skip input layer\n",
    "            for neuron in layer.neurons:\n",
    "                for w in range(len(neuron.weights)):\n",
    "                    neuron.weights[w] = neuron.weights[w] + (neuron.learning_rate\n",
    "        * (layer.previous_layer.output_cache[w]) * neuron.delta)\n",
    "                    \n",
    "    # train() uses the results of outputs() run over many inputs and compared\n",
    "    # against expecteds to feed backpropagate() and update_weights()\n",
    "    def train(self, inputs: List[List[float]], expecteds: List[List[float]]) -> None:\n",
    "        for location, xs in enumerate(inputs):\n",
    "            ys: List[float] = expecteds[location]\n",
    "            outs: List[float] = self.outputs(xs)\n",
    "            self.backpropagate(ys)\n",
    "            self.update_weights()\n",
    "            \n",
    "    # for generalized results that require classification\n",
    "    # this function will return the correct number of trials\n",
    "    # and the percentage correct out of the total\n",
    "    def validate(self, inputs: List[List[float]], expecteds: List[T], interpret_output: \n",
    "                 Callable[[List[float]], T]) -> Tuple[int, int, float]:\n",
    "        correct: int = 0\n",
    "        for input, expected in zip(inputs, expecteds):\n",
    "            result: T = interpret_output(self.outputs(input))\n",
    "            if result == expected:\n",
    "                correct += 1\n",
    "        percentage: float = correct / len(inputs)\n",
    "        return correct, len(inputs), percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification problems \n",
    "There are many machine-learning techniques that can be used for classification problems. Perhaps you have heard of support vector machines, decision trees, or naive Bayes classifiers. (There are others, too.) Recently, neural networks have become widely deployed in the classification space. They are more computationally intensive than some of the other classification algorithms, but their ability to classify seemingly arbitrary kinds of data makes them a powerful technique. \n",
    "\n",
    "### Normalizing data \n",
    "The data sets that we want to work with generally require some “cleaning” before they are input into our algorithms. Cleaning may involve removing extraneous characters, deleting duplicates, fixing errors, and other menial tasks. The aspect of cleaning we will need to perform for the two data sets we are working with is normalization. Normalization is about taking attributes recorded on different scales and converting them to a common scale.\n",
    "\n",
    "Every neuron in our network outputs values between 0 and 1 due to the sigmoid activation function. It sounds logical that a scale between 0 and 1 would make sense for the attributes in our input data set as well. Converting a scale from some range to a range between 0 and 1 is not challenging. For any value, V, in a particular attribute range with maximum, max, and minimum, min, the formula is just newV = (oldV - min) / (max - min). This operation is known as feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume all rows are of equal length\n",
    "# and feature scale each column to be in the range 0 - 1\n",
    "def normalize_by_feature_scaling(dataset: List[List[float]]) -> None:\n",
    "    for col_num in range(len(dataset[0])):\n",
    "        column: List[float] = [row[col_num] for row in dataset]\n",
    "        maximum = max(column)\n",
    "    minimum = min(column)\n",
    "    for row_num in range(len(dataset)):\n",
    "        dataset[row_num][col_num] = (dataset[row_num][col_num] - minimum) / (maximum - minimum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The classic iris data set \n",
    "Originally collected in the 1930s, the data set consists of 150 samples of iris plants (pretty flowers), split amongst three different species (50 of each). Each plant is measured on four different attributes: sepal length, sepal width, petal length, and petal width. \n",
    "\n",
    "The iris data set is from the University of California’s UCI Machine Learning Repository: M. Lichman, UCI Machine Learning Repository (Irvine, CA: University of California, School of Information and Computer Science, 2013).\n",
    "\n",
    "Each line represents one data point. The four numbers represent the four attributes (sepal length, sepal width, petal length, and petal width), which, again, are arbitrary to us in terms of what they actually represent. The name at the end of each line represents the particular iris species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from typing import List\n",
    "from random import shuffle\n",
    "\n",
    "def read_lines(file_name: str, **fmtparams) -> List:\n",
    "    lines = []\n",
    "    with open(file_name, mode='r') as csv_file:\n",
    "        lines: List = list(csv.reader(csv_file, fmtparams))\n",
    "        shuffle(lines) # get our lines of data in random order\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 correct of 10 = 90.0%\n"
     ]
    }
   ],
   "source": [
    "iris_parameters: List[List[float]] = []\n",
    "iris_classifications: List[List[float]] = []\n",
    "iris_species: List[str] = []\n",
    "irises: List = read_lines('iris.csv')\n",
    "for iris in irises:\n",
    "    parameters: List[float] = [float(n) for n in iris[0:4]]\n",
    "    iris_parameters.append(parameters)\n",
    "    species: str = iris[4]\n",
    "    if species == \"Iris-setosa\":\n",
    "        iris_classifications.append([1.0, 0.0, 0.0])\n",
    "    elif species == \"Iris-versicolor\":\n",
    "        iris_classifications.append([0.0, 1.0, 0.0])\n",
    "    else:\n",
    "        iris_classifications.append([0.0, 0.0, 1.0])\n",
    "    iris_species.append(species)\n",
    "\n",
    "normalize_by_feature_scaling(iris_parameters)\n",
    "\n",
    "iris_network: Network = Network([4, 6, 3], 0.3)\n",
    "    \n",
    "def iris_interpret_output(output: List[float]) -> str:\n",
    "    if max(output) == output[0]:\n",
    "        return \"Iris-setosa\"\n",
    "    elif max(output) == output[1]:\n",
    "        return \"Iris-versicolor\"\n",
    "    else:\n",
    "        return \"Iris-virginica\"\n",
    "    \n",
    "# train over the first 140 irises in the data set 50 times\n",
    "iris_trainers: List[List[float]] = iris_parameters[0:140]\n",
    "iris_trainers_corrects: List[List[float]] = iris_classifications[0:140]\n",
    "for _ in range(50):\n",
    "    iris_network.train(iris_trainers, iris_trainers_corrects)\n",
    "    \n",
    "# test over the last 10 of the irises in the data set\n",
    "iris_testers: List[List[float]] = iris_parameters[140:150]\n",
    "iris_testers_corrects: List[str] = iris_species[140:150]\n",
    "iris_results = iris_network.validate(iris_testers, iris_testers_corrects,\n",
    "     iris_interpret_output)\n",
    "print(f\"{iris_results[0]} correct of {iris_results[1]} = {iris_results[2] * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying wine \n",
    "We are going to test our neural network with another data set, one based on the chemical analysis of wine cultivars from Italy. There are 178 samples in the data set.\n",
    "\n",
    "The layer configuration for the wine-classification network needs 13 input neurons, as was already mentioned (one for each parameter). It also needs three output neurons. (There are three cultivars of wine, just as there were three species of iris.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 correct of 28 = 46.42857142857143%\n"
     ]
    }
   ],
   "source": [
    "wine_parameters: List[List[float]] = []\n",
    "wine_classifications: List[List[float]] = []\n",
    "wine_species: List[int] = []\n",
    "wines: List = read_lines('wine.csv', quoting=csv.QUOTE_NONNUMERIC)\n",
    "for wine in wines:\n",
    "    parameters: List[float] = [float(n) for n in wine[1:14]]\n",
    "    wine_parameters.append(parameters)\n",
    "    species: int = int(wine[0])\n",
    "    if species == 1:\n",
    "        wine_classifications.append([1.0, 0.0, 0.0])\n",
    "    elif species == 2:\n",
    "        wine_classifications.append([0.0, 1.0, 0.0])\n",
    "    else:\n",
    "        wine_classifications.append([0.0, 0.0, 1.0])\n",
    "    wine_species.append(species)\n",
    "        \n",
    "normalize_by_feature_scaling(wine_parameters)\n",
    "\n",
    "wine_network: Network = Network([13, 13, 7, 3], 0.5)\n",
    "\n",
    "def wine_interpret_output(output: List[float]) -> int:\n",
    "    if max(output) == output[0]:\n",
    "        return 1\n",
    "    elif max(output) == output[1]:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "# train \n",
    "MAX = 150\n",
    "wine_trainers: List[List[float]] = wine_parameters[0:MAX]\n",
    "wine_trainers_corrects: List[List[float]] = wine_classifications[0:MAX]\n",
    "for _ in range(20):\n",
    "    wine_network.train(wine_trainers, wine_trainers_corrects)\n",
    "\n",
    "# test over the last 28 of the wines in the data set\n",
    "wine_testers: List[List[float]] = wine_parameters[150:178]\n",
    "wine_testers_corrects: List[int] = wine_species[150:178]\n",
    "wine_results = wine_network.validate(wine_testers, wine_testers_corrects, wine_interpret_output)\n",
    "print(f\"{wine_results[0]} correct of {wine_results[1]} = {wine_results[2] * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
